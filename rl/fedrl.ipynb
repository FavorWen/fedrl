{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from helper import criterion\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rl_model import *\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim 52, act_dim 25\n"
     ]
    }
   ],
   "source": [
    "client_nums = 25\n",
    "participant_nums = 5\n",
    "history_dim = 2\n",
    "obs_dim = history_dim * (client_nums + 1)\n",
    "seed = 0\n",
    "device = 'cuda'\n",
    "dataset_name = 'CIFAR10'\n",
    "arch_name = 'CNN'\n",
    "partition = 'iid'\n",
    "optimizer = 'SGD'\n",
    "lr = 0.1\n",
    "epoch = 1\n",
    "rl_ddl = 100\n",
    "batch_size = 32\n",
    "device = 'cpu'\n",
    "\n",
    "act_dim = client_nums\n",
    "\n",
    "print('obs_dim {}, act_dim {}'.format(obs_dim, act_dim))\n",
    "\n",
    "model = Model(obs_dim, act_dim).to('cuda')\n",
    "alg = PolicyGradient(model, lr, device=device)\n",
    "agent = Agent(alg, obs_dim=obs_dim, act_dim=act_dim, participant_nums=participant_nums, client_nums=client_nums, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Env(obs_dim, arch_name, client_nums, participant_nums, dataset_name, partition, seed, device, criterion, optimizer=\"SGD\", lr=lr, epoch=epoch, rl_ddl = rl_ddl, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = torch.ones(batch_size, 1)\n",
    "rewards[0] = 2\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(batch_size, 1, obs_dim).to('cuda')\n",
    "actions = torch.ones(batch_size, participant_nums, dtype=torch.long)\n",
    "rewards = torch.ones(batch_size, 1)\n",
    "for i in range(batch_size):\n",
    "    act = np.random.choice(range(client_nums), size=participant_nums, replace=False)\n",
    "    act = torch.Tensor(act)\n",
    "    actions[i] = act\n",
    "y = model(x).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots = torch.zeros(batch_size, act_dim).scatter(1, torch.LongTensor(actions), torch.ones(batch_size, act_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.squeeze()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), tensor(0., grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.mean(torch.sum(torch.log(y.squeeze()) * one_hots * rewards, dim=1))\n",
    "c.shape,c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a) == type(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "21:00:36 - root - INFO - Tick 1 Spearman co: -0.02255025502550255\n",
      "21:00:42 - root - INFO - Tick 2 Spearman co: 0.06567056705670567\n",
      "21:00:48 - root - INFO - Tick 3 Spearman co: -0.1125112511251125\n",
      "21:00:53 - root - INFO - Tick 4 Spearman co: -0.024050405040504052\n",
      "21:00:59 - root - INFO - Tick 5 Spearman co: -0.083012301230123\n",
      "21:01:05 - root - INFO - Tick 6 Spearman co: 0.027014701470147014\n",
      "21:01:11 - root - INFO - Tick 7 Spearman co: 0.055733573357335725\n",
      "21:01:17 - root - INFO - Tick 8 Spearman co: -0.06708670867086708\n",
      "21:01:23 - root - INFO - Tick 9 Spearman co: 0.009864986498649865\n",
      "21:01:29 - root - INFO - Tick 10 Spearman co: -0.1242004200420042\n",
      "21:01:35 - root - INFO - Tick 11 Spearman co: -0.0691029102910291\n",
      "21:01:41 - root - INFO - Tick 12 Spearman co: -0.016645664566456646\n",
      "21:01:47 - root - INFO - Tick 13 Spearman co: -0.09044104410441044\n",
      "21:01:53 - root - INFO - Tick 14 Spearman co: 0.05422142214221421\n",
      "21:01:59 - root - INFO - Tick 15 Spearman co: -0.1082268226822682\n",
      "21:02:05 - root - INFO - Tick 16 Spearman co: 0.03859585958595859\n",
      "21:02:11 - root - INFO - Tick 17 Spearman co: -0.07216321632163217\n",
      "21:02:17 - root - INFO - Tick 18 Spearman co: 0.09875787578757873\n",
      "21:02:22 - root - INFO - Tick 19 Spearman co: -0.1561116111611161\n",
      "21:02:27 - root - INFO - Tick 20 Spearman co: -0.00534053405340534\n",
      "21:02:33 - root - INFO - Tick 21 Spearman co: 0.04514851485148514\n",
      "21:02:39 - root - INFO - Tick 22 Spearman co: -0.010633063306330631\n",
      "21:02:45 - root - INFO - Tick 23 Spearman co: -0.10363036303630362\n",
      "21:02:51 - root - INFO - Tick 24 Spearman co: -0.045112511251125115\n",
      "21:02:57 - root - INFO - Tick 25 Spearman co: -0.09665766576657665\n",
      "21:03:02 - root - INFO - Tick 26 Spearman co: -0.027638763876387636\n",
      "21:03:09 - root - INFO - Tick 27 Spearman co: -0.06322232223222322\n",
      "21:03:15 - root - INFO - Tick 28 Spearman co: -0.0039003900390039005\n",
      "21:03:21 - root - INFO - Tick 29 Spearman co: -0.12176417641764176\n",
      "21:03:26 - root - INFO - Tick 30 Spearman co: 0.01194119411941194\n",
      "21:03:32 - root - INFO - Tick 31 Spearman co: 0.03483948394839484\n",
      "21:03:38 - root - INFO - Tick 32 Spearman co: -0.09461746174617461\n",
      "21:03:45 - root - INFO - Tick 33 Spearman co: -0.05329732973297329\n",
      "21:03:51 - root - INFO - Tick 34 Spearman co: 0.05693369336933693\n",
      "21:03:56 - root - INFO - Tick 35 Spearman co: -0.06351035103510351\n",
      "21:04:02 - root - INFO - Tick 36 Spearman co: 0.007176717671767176\n",
      "21:04:08 - root - INFO - Tick 37 Spearman co: -0.0417041704170417\n",
      "21:04:14 - root - INFO - Tick 38 Spearman co: 0.04962496249624962\n",
      "21:04:19 - root - INFO - Tick 39 Spearman co: -0.05357335733573356\n",
      "21:04:25 - root - INFO - Tick 40 Spearman co: 0.0032403240324032404\n",
      "21:04:31 - root - INFO - Tick 41 Spearman co: 0.009228922892289227\n",
      "21:04:38 - root - INFO - Tick 42 Spearman co: -0.14385838583858385\n",
      "21:04:44 - root - INFO - Tick 43 Spearman co: -0.11062706270627061\n",
      "21:04:49 - root - INFO - Tick 44 Spearman co: -0.06948694869486949\n",
      "21:04:55 - root - INFO - Tick 45 Spearman co: -0.0033243324332433237\n",
      "21:05:01 - root - INFO - Tick 46 Spearman co: -0.057425742574257414\n",
      "21:05:08 - root - INFO - Tick 47 Spearman co: -0.01255325532553255\n",
      "21:05:14 - root - INFO - Tick 48 Spearman co: -0.07707170717071707\n",
      "21:05:19 - root - INFO - Tick 49 Spearman co: -0.07781578157815781\n",
      "21:05:25 - root - INFO - Tick 50 Spearman co: -0.12242424242424242\n",
      "21:05:31 - root - INFO - Tick 51 Spearman co: -0.013933393339333931\n",
      "21:05:38 - root - INFO - Tick 52 Spearman co: -0.058169816981698166\n",
      "21:05:43 - root - INFO - Tick 53 Spearman co: -0.06313831383138313\n",
      "21:05:48 - root - INFO - Tick 54 Spearman co: 0.04861686168616861\n",
      "21:05:54 - root - INFO - Tick 55 Spearman co: -0.12376837683768374\n",
      "21:06:00 - root - INFO - Tick 56 Spearman co: -0.06241824182418241\n",
      "21:06:06 - root - INFO - Tick 57 Spearman co: 0.012937293729372936\n",
      "21:06:12 - root - INFO - Tick 58 Spearman co: -0.0554095409540954\n",
      "21:06:18 - root - INFO - Tick 59 Spearman co: -0.06127812781278127\n",
      "21:06:24 - root - INFO - Tick 60 Spearman co: -0.003228322832283228\n",
      "21:06:30 - root - INFO - Tick 61 Spearman co: -0.08706870687068706\n",
      "21:06:36 - root - INFO - Tick 62 Spearman co: 0.016273627362736274\n",
      "21:06:42 - root - INFO - Tick 63 Spearman co: 0.018997899789978994\n",
      "21:06:48 - root - INFO - Tick 64 Spearman co: -0.11741974197419741\n",
      "21:06:54 - root - INFO - Tick 65 Spearman co: -0.14672667266726672\n",
      "21:07:00 - root - INFO - Tick 66 Spearman co: 0.04674467446744674\n",
      "21:07:06 - root - INFO - Tick 67 Spearman co: -0.02203420342034203\n",
      "21:07:12 - root - INFO - Tick 68 Spearman co: 0.0030243024302430243\n",
      "21:07:18 - root - INFO - Tick 69 Spearman co: -0.010021002100210018\n",
      "21:07:23 - root - INFO - Tick 70 Spearman co: 0.02472247224722472\n",
      "21:07:29 - root - INFO - Tick 71 Spearman co: -0.005376537653765376\n",
      "21:07:35 - root - INFO - Tick 72 Spearman co: 0.02994299429942994\n",
      "21:07:41 - root - INFO - Tick 73 Spearman co: -0.08158415841584157\n",
      "21:07:47 - root - INFO - Tick 74 Spearman co: -0.03147914791479148\n",
      "21:07:53 - root - INFO - Tick 75 Spearman co: -0.08169216921692168\n",
      "21:07:59 - root - INFO - Tick 76 Spearman co: -0.0044164416441644155\n",
      "21:08:05 - root - INFO - Tick 77 Spearman co: -0.03263126312631263\n",
      "21:08:11 - root - INFO - Tick 78 Spearman co: -0.02018601860186018\n",
      "21:08:17 - root - INFO - Tick 79 Spearman co: -0.0498049804980498\n",
      "21:08:23 - root - INFO - Tick 80 Spearman co: -0.07849984998499848\n",
      "21:08:29 - root - INFO - Tick 81 Spearman co: -0.040456045604560446\n",
      "21:08:35 - root - INFO - Tick 82 Spearman co: -0.09906990699069906\n",
      "21:08:41 - root - INFO - Tick 83 Spearman co: -0.05801380138013801\n",
      "21:08:46 - root - INFO - Tick 84 Spearman co: 0.009264926492649263\n",
      "21:08:52 - root - INFO - Tick 85 Spearman co: -0.16105610561056105\n",
      "21:08:59 - root - INFO - Tick 86 Spearman co: -0.10133813381338133\n",
      "21:09:04 - root - INFO - Tick 87 Spearman co: -0.1279327932793279\n",
      "21:09:09 - root - INFO - Tick 88 Spearman co: -0.05212121212121212\n",
      "21:09:15 - root - INFO - Tick 89 Spearman co: -0.062310231023102305\n",
      "21:09:21 - root - INFO - Tick 90 Spearman co: 0.006576657665766576\n",
      "21:09:28 - root - INFO - Tick 91 Spearman co: -0.06131413141314131\n",
      "21:09:34 - root - INFO - Tick 92 Spearman co: -0.02142214221422142\n",
      "21:09:39 - root - INFO - Tick 93 Spearman co: -0.01784578457845784\n",
      "21:09:45 - root - INFO - Tick 94 Spearman co: -0.10612661266126612\n",
      "21:09:51 - root - INFO - Tick 95 Spearman co: -0.09406540654065405\n",
      "21:09:58 - root - INFO - Tick 96 Spearman co: -0.046420642064206416\n",
      "21:10:04 - root - INFO - Tick 97 Spearman co: -0.03317131713171317\n",
      "21:10:09 - root - INFO - Tick 98 Spearman co: -0.018277827782778276\n",
      "21:10:15 - root - INFO - Tick 99 Spearman co: 0.027038703870387034\n",
      "21:10:21 - root - INFO - Tick 100 Spearman co: -0.023006300630063006\n"
     ]
    }
   ],
   "source": [
    "obs_list, action_list, reward_list = run_episode(env, agent)\n",
    "batch_obs = obs_list\n",
    "batch_action = action_list\n",
    "batch_reward = calc_reward_to_go(reward_list)\n",
    "\n",
    "agent.learn(batch_obs, batch_action, batch_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_prob =  agent.predict(env.state).detach().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(act_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/HARD-DRIVE/QI/rl/fedrl.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B3090-/HARD-DRIVE/QI/rl/fedrl.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m obs_list, action_list, reward_list \u001b[39m=\u001b[39m run_episode(env, agent)\n",
      "File \u001b[0;32m/HARD-DRIVE/QI/rl/train.py:15\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     obs_list\u001b[39m.\u001b[39mappend(obs)\n\u001b[0;32m---> 15\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49msample(obs) \u001b[39m# 采样动作\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     action_list\u001b[39m.\u001b[39mappend(action)\n\u001b[1;32m     18\u001b[0m     obs, reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/HARD-DRIVE/QI/rl/rl_model.py:71\u001b[0m, in \u001b[0;36mAgent.sample\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[0;32m---> 71\u001b[0m     act_prob \u001b[39m=\u001b[39m  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(obs)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     72\u001b[0m     act \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_dim), size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparticipant_nums, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, p\u001b[39m=\u001b[39mact_prob)\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m act\n",
      "File \u001b[0;32m/HARD-DRIVE/QI/rl/rl_model.py:76\u001b[0m, in \u001b[0;36mAgent.predict\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malgo\u001b[39m.\u001b[39;49mpredict(obs)\n",
      "File \u001b[0;32m/HARD-DRIVE/QI/rl/rl_model.py:33\u001b[0m, in \u001b[0;36mPolicyGradient.predict\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(obs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/HARD-DRIVE/QI/rl/rl_model.py:23\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[1;32m     22\u001b[0m     obs \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(obs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "obs_list, action_list, reward_list = run_episode(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Test reward: 0.6296124767150197, Spearman co: -0.09855385538553854\n"
     ]
    }
   ],
   "source": [
    "batch_obs = obs_list\n",
    "batch_action = action_list\n",
    "batch_reward = calc_reward_to_go(reward_list)\n",
    "\n",
    "agent.learn(batch_obs, batch_action, batch_reward)\n",
    "\n",
    "total_reward = evaluate(env, agent, render=False) # render=True 查看渲染效果，需要在本地运行，AIStudio无法显示\n",
    "spearman_co = spearman(env, agent)\n",
    "print('Test reward: {}, Spearman co: {}'.format(total_reward, spearman_co))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alg.model.train()\n",
    "# cost = 0\n",
    "# for i in range(len(obs_list)):\n",
    "#     obs = obs_list[i].detach()\n",
    "#     action = action_list[i]\n",
    "#     reward = reward_list[i]\n",
    "\n",
    "#     act_prob = alg.predict(obs).squeeze()\n",
    "#     act_dim = alg.model.act_dim\n",
    "#     one_hot = torch.zeros(act_dim).scatter(0, torch.LongTensor(action), torch.ones(act_dim))\n",
    "#     cost += torch.sum(-1 * torch.log(act_prob) * one_hot * reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
